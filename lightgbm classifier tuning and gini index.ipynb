{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7e7b4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83f3e485",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_parquet(r\"C:\\Users\\beher\\jupyterZ\\Amex_credit\\cleaned.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "246cf826",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df[['B_9', 'D_75', 'D_58', 'B_7', 'B_23', 'B_4', 'B_1', 'B_11', 'R_1',\n",
    "       'R_3', 'R_2', 'P_4', 'R_10', 'B_28', 'R_4', 'S_15', 'D_39', 'R_5',\n",
    "       'R_8']]\n",
    "y=df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49d7c160",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, train_size=0.98, shuffle=True, random_state=100)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, train_size=0.5,shuffle=True,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd99897c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 4070410, number of negative: 4070536\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.724329 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 8140946, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(colsample_bytree=0.8, learning_rate=0.01, max_depth=15,\n",
       "               subsample=0.6)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(colsample_bytree=0.8, learning_rate=0.01, max_depth=15,\n",
       "               subsample=0.6)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier(colsample_bytree=0.8, learning_rate=0.01, max_depth=15,\n",
       "               subsample=0.6)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming you have your data loaded in X_train, y_train, X_test, and y_test\n",
    "\n",
    "# Initialize the LightGBM Classifier\n",
    "lgb_classifier = lgb.LGBMClassifier(n_estimators=100, max_depth=15, learning_rate=0.01, subsample=0.6, colsample_bytree=0.8)\n",
    "\n",
    "# Train the model on the training data\n",
    "lgb_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdc89cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "LightGBM Classifier Train Score is: 0.8288488831641925\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "LightGBM Classifier Test Score is: 0.8307833058468057\n"
     ]
    }
   ],
   "source": [
    "# Calculate details\n",
    "print('LightGBM Classifier Train Score is:', lgb_classifier.score(X_train, y_train))\n",
    "print('LightGBM Classifier Test Score is:', lgb_classifier.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12e6fb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 33334, number of negative: 33122\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006401 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 66456, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501595 -> initscore=0.006380\n",
      "[LightGBM] [Info] Start training from score 0.006380\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 33334, number of negative: 33123\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006252 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 66457, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501587 -> initscore=0.006350\n",
      "[LightGBM] [Info] Start training from score 0.006350\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 33334, number of negative: 33123\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007662 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 66457, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501587 -> initscore=0.006350\n",
      "[LightGBM] [Info] Start training from score 0.006350\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 33335, number of negative: 33122\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006469 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 66457, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501603 -> initscore=0.006410\n",
      "[LightGBM] [Info] Start training from score 0.006410\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 33335, number of negative: 33122\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006918 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 66457, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501603 -> initscore=0.006410\n",
      "[LightGBM] [Info] Start training from score 0.006410\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Train Score Value: [0.83238534 0.83118407 0.83150007 0.83178597 0.83207187]\n",
      "Test Score Value: [0.8228709  0.83050439 0.83152763 0.82827736 0.82797641]\n",
      "Fit Time: [0.58453298 0.61712146 0.69947863 0.58570957 0.59043789]\n",
      "Score Time: [0.03164315 0.03896761 0.04743218 0.04735136 0.04815221]\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation\n",
    "CrossValidateValues4 = cross_validate(lgb_classifier, X_val, y_val, cv=5, return_train_score=True)\n",
    "\n",
    "# Showing Results\n",
    "print('Train Score Value:', CrossValidateValues4['train_score'])\n",
    "print('Test Score Value:', CrossValidateValues4['test_score'])\n",
    "print('Fit Time:', CrossValidateValues4['fit_time'])\n",
    "print('Score Time:', CrossValidateValues4['score_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c046e610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix is\n",
      " [[32718  8887]\n",
      " [ 5170 36296]]\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set\n",
    "y_pred_lgb = lgb_classifier.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "CM_lgb = confusion_matrix(y_test, y_pred_lgb)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(4, 3))\n",
    "sns.heatmap(CM_lgb, center=True, annot=True, fmt=\"d\")\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "print('Confusion Matrix is\\n', CM_lgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092e95fa",
   "metadata": {},
   "source": [
    "# Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e05ad67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256428\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.484867 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512756, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.501851 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.541074 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.566029 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.539966 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256428\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.140336 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512756, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.517392 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.436855 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.507262 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.507535 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256428\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.626099 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512756, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.853710 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.807888 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.782278 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.750508 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256428\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.779814 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512756, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.799484 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.708281 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.736360 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.956429 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256428\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.766287 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512756, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.729559 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.769580 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.733905 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.740816 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256428\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.232684 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512756, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.224625 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.696492 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.288959 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.714279 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256428\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.553054 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512756, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.636471 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.770777 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.730392 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.216924 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256428\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.690061 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512756, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.771425 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.695395 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.707471 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.625118 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256428\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.633353 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512756, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.161568 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.637724 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.615075 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.515389 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256428\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.705094 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512756, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.432613 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.418845 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.429804 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 3256328, number of negative: 3256429\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.440907 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 6512757, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 4070410, number of negative: 4070536\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.578266 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4845\n",
      "[LightGBM] [Info] Number of data points in the train set: 8140946, number of used features: 19\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499992 -> initscore=-0.000031\n",
      "[LightGBM] [Info] Start training from score -0.000031\n",
      "Best Hyperparameters: {'subsample': 0.7, 'n_estimators': 150, 'max_depth': 10, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Best Train Score: 0.8406854682490217\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Best Test Score: 0.8423035716435339\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have your data loaded in X_train, y_train, X_test, and y_test\n",
    "\n",
    "# Initialize the LightGBM Classifier\n",
    "lgb_classifier = lgb.LGBMClassifier()\n",
    "\n",
    "# Define the hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.6, 0.7, 0.8],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8]\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV with the LightGBM Classifier and hyperparameter grid\n",
    "random_search = RandomizedSearchCV(estimator=lgb_classifier, param_distributions=param_grid, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
    "\n",
    "# Perform the random search for hyperparameter tuning on the training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best model from the random search\n",
    "best_params = random_search.best_params_\n",
    "best_lgb_classifier = random_search.best_estimator_\n",
    "\n",
    "# Print the best parameters and train/test scores\n",
    "print('Best Hyperparameters:', best_params)\n",
    "print('Best Train Score:', best_lgb_classifier.score(X_train, y_train))\n",
    "print('Best Test Score:', best_lgb_classifier.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73bc9ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "799da6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Mean Squared Error: 0.11416548621828866\n",
      "Best Hyperparameters: {'subsample': 0.7, 'n_estimators': 150, 'max_depth': 10, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n"
     ]
    }
   ],
   "source": [
    "y_probs = best_lgb_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the Mean Squared Error (MSE) on the test set\n",
    "mse = mean_squared_error(y_test, y_probs)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Best Hyperparameters:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e5d9931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.49345365, 0.01675468, 0.24981508, ..., 0.02050006, 0.95587156,\n",
       "       0.51141851])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f09433f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0b09e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"prediction\":np.array(y_probs), \"target\": np.array(y_test)}  \n",
    "df_gini=pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0382eceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prediction</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.493454</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.016755</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.249815</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.627894</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.972453</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83066</th>\n",
       "      <td>0.810095</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83067</th>\n",
       "      <td>0.813091</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83068</th>\n",
       "      <td>0.020500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83069</th>\n",
       "      <td>0.955872</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83070</th>\n",
       "      <td>0.511419</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83071 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       prediction  target\n",
       "0        0.493454       1\n",
       "1        0.016755       0\n",
       "2        0.249815       0\n",
       "3        0.627894       1\n",
       "4        0.972453       1\n",
       "...           ...     ...\n",
       "83066    0.810095       1\n",
       "83067    0.813091       1\n",
       "83068    0.020500       0\n",
       "83069    0.955872       1\n",
       "83070    0.511419       0\n",
       "\n",
       "[83071 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a14ee1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19985</th>\n",
       "      <td>1</td>\n",
       "      <td>0.997405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4328</th>\n",
       "      <td>1</td>\n",
       "      <td>0.997249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23064</th>\n",
       "      <td>1</td>\n",
       "      <td>0.997154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50370</th>\n",
       "      <td>1</td>\n",
       "      <td>0.997139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54343</th>\n",
       "      <td>1</td>\n",
       "      <td>0.997098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70169</th>\n",
       "      <td>0</td>\n",
       "      <td>0.004421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72608</th>\n",
       "      <td>0</td>\n",
       "      <td>0.004309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38569</th>\n",
       "      <td>0</td>\n",
       "      <td>0.004278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58541</th>\n",
       "      <td>0</td>\n",
       "      <td>0.004251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54985</th>\n",
       "      <td>0</td>\n",
       "      <td>0.004245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83071 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       target  prediction\n",
       "19985       1    0.997405\n",
       "4328        1    0.997249\n",
       "23064       1    0.997154\n",
       "50370       1    0.997139\n",
       "54343       1    0.997098\n",
       "...       ...         ...\n",
       "70169       0    0.004421\n",
       "72608       0    0.004309\n",
       "38569       0    0.004278\n",
       "58541       0    0.004251\n",
       "54985       0    0.004245\n",
       "\n",
       "[83071 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = (pd.concat([df_gini[\"target\"], df_gini[\"prediction\"]], axis='columns').sort_values('prediction', ascending=False))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30b1eccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def amex_metric(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "\n",
    "    def top_four_percent_captured(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        four_pct_cutoff = int(0.04 * df['weight'].sum())\n",
    "        df['weight_cumsum'] = df['weight'].cumsum()\n",
    "        df_cutoff = df.loc[df['weight_cumsum'] <= four_pct_cutoff]\n",
    "        return (df_cutoff['target'] == 1).sum() / (df['target'] == 1).sum()\n",
    "        \n",
    "    def weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        \n",
    "        # Instead of going through the population from poorest to richest, we go through our predictions from lowest to highest.?? (other way round)\n",
    "        # Sort the actual values by the predictions\n",
    "        df = (pd.concat([y_true, y_pred], axis='columns')\n",
    "              .sort_values('prediction', ascending=False))\n",
    "        \n",
    "        # We assign the weight to the majority class (0) that we subsample\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        \n",
    "        df['random'] = (df['weight'] / df['weight'].sum()).cumsum()\n",
    "        \n",
    "        # Sum up the targets\n",
    "        total_pos = (df['target'] * df['weight']).sum()\n",
    "        \n",
    "         # Instead of summing up the income, we sum up the actual values of our predictions:\n",
    "        # Sum up the actual values\n",
    "        # This corresponds to the Lorenz Curve\n",
    "        df['cum_pos_found'] = (df['target'] * df['weight']).cumsum()\n",
    "        df['lorentz'] = df['cum_pos_found'] / total_pos\n",
    "        \n",
    "        df['gini'] = (df['lorentz'] - df['random']) * df['weight']\n",
    "        return df['gini'].sum()\n",
    "\n",
    "    def normalized_weighted_gini(y_true: pd.DataFrame, y_pred: pd.DataFrame) -> float:\n",
    "        y_true_pred = y_true.rename(columns={'target': 'prediction'})\n",
    "        return weighted_gini(y_true, y_pred) / weighted_gini(y_true, y_true_pred)\n",
    "\n",
    "    g = normalized_weighted_gini(y_true, y_pred)\n",
    "    d = top_four_percent_captured(y_true, y_pred)\n",
    "\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c57944a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6006170739766306"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amex_metric(df[['target']], df[['prediction']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55b92ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
